---
layout: post
title: Series Unpacking Interview Questions - Jacob Kaplan-Moss
date: 2021-02-19
issue_number: 62
original_url: https://www.researchcomputingteams.org/newsletter_issues/0062
tags: ['hiring', 'interviewing_and_evaluating']
priority: 1
---

<!-- markdownlint-disable MD033 -->
<!-- markdownlint-disable MD041 -->
<!-- markdownlint-disable MD049 -->

[Series: Unpacking Interview Questions](https://jacobian.org/series/unpacking-interview-questions/) - Jacob Kaplan-Moss

This series of questions ties in very nicely with last week’s discsussion of evaluating against requirements.   Kaplan-Moss has five interview questions that he covers - maybe they’d be good for your roles, maybe not - with very carefully thought out rubrics for each:

1. The **value or skill** that the question is designed to measure.
2. **The question** itself
3. **Follow-ups** to ask
4. **Behaviors** to look for
5. **Positive signs / red flags** in the answer

Getting to this level for your own interviews will take a lot of work, and may never happen for some requirements if, as for most of us, your requirements change more rapidly than you hire.  But this standard is an excellent one to aspire to, whether your approach to evaluating against a particular requirement is an interview question or a skill-testing simulation of work product like a technical test.  What constitutes a good response, and a poor one - and if you can’t clearly express an answer to that, is it really a good evaluation method?  How should you follow up on a response/submission?