---
layout: post
title: Quantifying Independently Reproducible Machine Learning - Edward Raff, writing at The Gradient
date: 2020-02-28
<<<<<<< HEAD
issue_number: 12
=======
>>>>>>> 0a34fe0... First go at item pages
original_url: https://www.researchcomputingteams.org/newsletter_issues/0012
tags: ['reproducibility', 'software_development']
priority: 3
---

<!-- markdownlint-disable MD033 -->
<!-- markdownlint-disable MD041 -->
<!-- markdownlint-disable MD049 -->

[Quantifying Independently Reproducible Machine Learning](https://thegradient.pub/independently-reproducible-machine-learning/) - Edward Raff, writing at The Gradient

We worry a lot about about replication and reproducibility in research computing.  In this article, the author — who attempted to independently replicate the results and basic methods in 255 (!!!) ML papers.  Crucial here is *independent* replication; it’s not enough to just run the code, but to implement independently.  He was successful 162 times.

That’s enough papers to do some quantitative analysis, and it’s interesting what aspects of the work were **not** correlated with successful independent replication.  A clearly written paper and answering emails was *much* more important than published source code or worked sample problems.