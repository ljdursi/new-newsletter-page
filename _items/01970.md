---
layout: post
title: Developing a modern data workflow for regularly updated data - Glenda M. Yenni *et al*, PLOS Biology<br>
date: 2022-03-19
issue_number: 114
original_url: https://www.researchcomputingteams.org/newsletter_issues/0114
tags: ['technical_leadership,ci_cd', 'technical_leadership,data_resources']
priority: 3
---

<!-- markdownlint-disable MD033 -->
<!-- markdownlint-disable MD041 -->
<!-- markdownlint-disable MD049 -->

[Developing a modern data workflow for regularly updated data](https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.3000125) - Glenda M. Yenni *et al*, PLOS Biology<br>
[Updating Data Recipe](https://www.updatingdata.org) - Ethan White, Albert Kim, and Glenda M. Yenni

This one’s a couple years old, and I’m surprised I hadn’t seen it before.

It’s getting easy to find good examples for scientists of getting started
with GitHub, and then to CI/CD, for code.  But for data it’s much harder.
And there's no reason why experimental data shouldn’t benefit from versioning,
and analysis pipeline CI/CD that code does.  As data gets cleaned up and the
pipeline matures, and data products start being released, these tools are
just as useful.

Here the authors publish a recipe, instructions, and template repos for [Github Actions](https://github.com/weecology/livedat) and [Travis CI](https://github.com/weecology/livedat) with data.  The immediate audience is for ecology, but the process is pretty general.  The instructions cover configuring the repo, connecting to Zenodo (for data artifacts) and the CI/CD tool.  Then data checks can be added (with the pipeline failing if the data breaks a validity constraint).  And data analysis can be done, with data products being published and versioned when a new release is created.  It’s really cool!

By making the things we want to see (data checking, proper data releases)
easier, as with automating them, we get more of what we want to see in the
world.  This is very useful work.
