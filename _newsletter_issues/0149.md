---
title: "#149 - 10 Dec 2022"
subtitle: "haring Expectations via Feedback; Debugging Groundhog Day; Incident Ground Rules and Practice; Research Application Managers; Spheres of Influence; Goals and Why's for the Year Ahead; Active Knowledge; Migrating Canva’s Data Stores"
date: 2022-12-10
layout: email
hero-img: https://www.researchcomputingteams.org/assets/images/issues/expectations_sm.png
---

<!-- markdownlint-disable MD033 -->
<!-- markdownlint-disable MD049 -->
<!-- markdownlint-disable MD036 -->

I got a couple of emails about last weeks issue, asking about both communicating expectations and team processes.  The most important, the most fundamental of these two are expectations.   In a team without a common understanding of expectation of each other, no process in the world will matter; they’re just so many words on paper.

With team members, peers, or colleagues, we share whether our expectations are met or not via feedback.

Feedback’s vital to calibrate and communicate shared expectations.  Feedback is how we acknowledge that people have done a good or great job, meeting or exceeding expectations; and it’s how we give people nudges or start a conversation when they haven’t met expectations.

The fact is we **all** want feedback, from *our* managers, *our* community, *our* peers.  We’re ambitious, driven people, and we want to know where the bar set is so we can clear it; we want recognition when we do clear it; and when we miss, we want to know so we can improve.   Working in an environment where you’re not getting feedback means knowing there’s a bar there, somewhere, but not being able to see or feel it, and so having any idea whether your jumps are high enough.  It’s disorienting and unsettling and unsatisfying.

(This is one of the reasons technical experts find it so hard to move to management.  When we’re doing our technical work, we’re constantly getting feedback of *some* kind, if only from the compiler or the cluster nodes or the data analysis pipelines.  As a manager, the feedback we get is much fuzzier, much less frequent, and much less reproducible).

We want teams where team members feel they can rely on each other; we want teams where the team members know that their manager will help them grow, and hold them accountable for areas they need to grow.  We do that by communicating shared expectations in a feedback culture, and the way we start doing *that* is by demonstrating how to give feedback in a helpful and respectful way.

The first important thing about good feedback is that it’s about the future.  There's an article in this week’s roundup about incident response: there, as here, the past is gone and beyond our reach.  We can’t do anything about it.  All we can do is learn from it, and use what we learn to incrementally improve the future.  We improve the future by nudging behaviours towards those that will have the impact desired.

The second important thing about good feedback is that it’s mostly positive.  Negative feedback is the hardest to give and receive, so that tends to be the focus of discussions about giving feedback - fair enough.  But most of the time, your peers and team members are meeting or expectations.  That’s worth calling out!  It’s arguably even *more* important to give feedback on meeting or exceeding expectations than for failing to meet them; there’s lots of ways to fall short of the bar, but only a few ways to meet or surpass it.  Just giving positive feedback regularly will have a remarkable impact, and there’s basically no reasonable amount of positive feedback that’s too much (*e.g.* #[64](https://www.researchcomputingteams.org/newsletter_issues/0064),  “A Simple Compliment Can Make a Big Difference”).  You can give positive feedback in precisely the same way as negative feedback, and for the same reasons - to make the future incrementally brighter by encouraging people to continue the good work they’re doing.

The third important thing about good feedback is that it is specific enough to be acted on.  Each piece of good feedback is about a single, discrete, action or behaviour that had some impact: good or bad, that exceeded or failed to meet expectations.  It’s focussed enough that people know what to do to meet expectations in the future.  “Good job on the presentation” or “it doesn’t seem like you took this work seriously” are equally useless as feedback.  What is the person hearing these things supposed to *do*?  At least “good job” briefly feels good to hear, but it doesn’t tell you anything about what you should do in the future to continue doing a good job.  And not taking this seriously - should you frown more when working on similar work next time?  Spend more time on it?  Which part of it?  And why?

Collectively, we know good ways to give feedback that meet these requirements — managers have converged on a model that works really well (e.g. #[120](https://www.researchcomputingteams.org/newsletter_issues/0120), Three Feedback Models).   The [Centre for Creative Leadership’s SBI model](https://www.ccl.org/articles/leading-effectively-articles/closing-the-gap-between-intent-vs-impact-sbii/) that Google uses, the [Manager-Tools Feedback Model](https://www.manager-tools.com/node/98050/view), and [Lara Hogan’s feedback equation](https://larahogan.me/blog/feedback-equation/) all follow a very similar structure:

![The feedback model: a focus on specific behaviour and the behaviour’s impact, bookended by questions.](https://www.researchcomputingteams.org/assets/images/issues/expectations_sm.png)

There’s a question to open the conversation.  That’s followed by a clear statement of a specific, concrete, objective behaviour and its impact, which are the core of all the models.  And finally it ends with a question.  Often that’s simply asking the recipient for a behaviour change (or to keep up the good work!).  Other times, if you suspect there’s lack of a shared understanding of goals and priorities, or something else in the way, it can usefully be an invitation to a discussion (“what were you trying to optimize for there?”) to uncover what was going on.

There’s multiple ways to escalate feedback if it doesn’t “take.”  But the vast majority of people you will ever work with *want* to meet expectations, whether you’re their manager or their peer, and will do so if given the chance.

That’s really all there is to it.  Feedback lands better if there’s already good lines of professional communications open, such as [one-on-ones](https://www.researchcomputingteams.org/one-on-ones).  And negative feedback will feel more reasonable and fair to people who are receiving positive feedback about the things they are doing well.  But developing a shared set of expectations via feedback is straightforward.  Like any new practice, it can feel like a bit of a struggle to get started with, and feel unnatural at first.  But it’s important, it’s worth doing, and your team members deserve to hear how they’re doing.

With that, on to the roundup!

## Managing Teams

[Debugging Teams: Groundhog Day](https://skamille.medium.com/debugging-teams-groundhog-day-f0483fea82d9) - Camille Fournier

> Have you ever been on a team that seemed to work very hard but never move forward? Where you look back quarter after quarter, or perhaps year after year, and you did a lot, but nothing actually seemed to happen?

As people of science, we know the difference between speed and velocity.  But from the inside of a team or organization that’s careening off madly without making consistent forward progress, it’s sometimes hard to tell them apart.

I write a lot, sometimes I think too much, about the importance of having a clear direction and prioritization, even though that makes for uncomfortable conversations sometimes.  But that’s the only sustainable way for our teams to have the impact they and our research communities deserve.  There are *always* more things that our teams *could* be doing than we have time to do.  For our efforts to add up to anything meaningful, they have to be focussed in a consistent direction.

Three "Groundhog Day” symptoms Fournier highlights hit uncomfortably close to some of the situations I’ve seen over the years:

> **The solution is always just around the corner.** Once we finish project X, things will be better. Once we sell to client Y, we’ll be set. Once we hire key hire Z, things will really take off.  […]
>
> **Addicted to ‘snacking’.  […]** ‘low-effort, low-impact work’. Teams stuck in Groundhog Day mode often find themselves snacking, working on easy projects that seem like they should deliver quick wins but never actually make a difference.
>
> **A lack of metrics and accountability. […]** You don’t learn anything project after project, so it’s almost as if each project doesn’t matter at all once it is completed. Without any metrics, you don’t know what is working and what isn’t working, and no one is held accountable for making sure that the work produced is actually meaningful.

The only solution to Groundhog Day, Fournier tells us, is to slow down, set a strategy that can be used to prioritize work (which means saying no to other work), understand why and how new tasks do or don’t fit into the strategy, and keep the goals in mind as the team starts to ~~speed back up~~ increase their velocity.

----------

This [HBR article](https://hbr.org/2022/12/no-remote-employees-arent-becoming-less-engaged) about remote employees collaboration patterns is interesting.  In a study of Zoom, Teams, and WebEx (!!) meetings from 10 large global companies, meetings between distributed team members who stayed with the company became more frequent, but also shorter, smaller (fewer attendees - by a factor of 2!), and more spontaneous.

As we’re all getting more used to remote work, we’re starting to figure out how to make it more effective, and it seems like we’re managing to reproduce some of the best parts of the in-office “hey, can we chat for a minute?” interactions.

----------

## Technical Leadership

[The Incident Retrospective Ground Rules](https://www.honeycomb.io/blog/incident-retrospective-ground-rules) - Lex Neva, Honeycomb<br/>
[Hiding Theory in Practice](https://ferd.ca/hiding-theory-in-practice.html) - Fred Hebert

Whether we’re managing projects or systems, things go wrong.  And as leads and managers, how we handle things going wrong shapes our entire team.  It affects how willing people are to raise issues, how willing they are to share with each other and ask questions, and how willing they are to try to improve things.

Neva describes how their first incident retrospective went at Honeycomb, starting with a meeting where the ground rules were read out (I’ve abbreviated them here):

- Our main objective is to learn, and get a better understanding of what it happened and what it could mean
- Improvement ideas or action items are great, and please feel free to note them down and share them later, but not during this meeting
- We are here to assume people wanted to do a good job, and that they did the best they could to meet objectives
- We’d like to avoid thinking about “what could we havee done” and instead think of “what can we do next time”
- Ask questions!  Especially if you think something is obvious to others but not to you
- Feedback about the session itself is more than welcome afterwards

Underlying these ground rules are some principles that have been long common elsewhere in incident response - a focus on learning versus action items, blame versus context, avoiding counterfactuals, and the importance of asking questions.   We know why these are important.  The past is fixed and immutable; assigning blame and otherwise making things miserable for team members accomplishes less than nothing.  And hypothesizing about what might have happened in other parallel-universe pasts gets us nowhere.

Our goal is always to incrementally improve the future, and we can only do that by being genuinely open to learning about what happened.   That way the entire team can learn from each other, and we can work together to make *this* way of things going wrong less likely in the future.

Hebert talks more explicitly about those principles, and some others (*e.g.* “human error” and “root cause” generally not being useful concepts).  Hebert’s article is nice because he discusses what to do when talking to someone who nonetheless steers into violations of those ground rules - assigning blame, “they should have known”, etc.

Assuming the discussion is one-on-one, Hebert says trying to police the reaction or ignore it isn’t helpful:

> Strong emotional reactions are as good data as any architecture diagram for your work. They can highlight important and significant dynamics about your organization. Ignoring them is ignoring potentially useful data, and may damage the trust people put in you.

This is good advice for managers and leads generally.  He gives several suggestions about potentially useful places to dig in to learn more when presented with these reactions.  (*e.g*., if this person is saying “it should have been obvious..” but it wasn’t obvious to the person they’re blaming, that “hints at a clash in their mental models, which is a great opportunity to compare and contrast them. Diverging perspectives like that are worth digging into because they can reveal a lot.”)

----------

## Managing Your Own Career

[What can you really influence? Find out by taking a look at your Sphere of Influence](https://www.kateleto.com/articles/sphere-of-influence) - Kate Leto

A disturbingly easy way for an RCT leader to burn themselves out is by shouldering responsibility for things they can’t possibly control.  (Ask me how I know!)

We’re ambitious people.  We get into this line of work because we want to push forward the boundaries of human knowledge.  There’s limitless places our teams’ expertise can be applied, and limited time to do so.  So when we see barriers to what we know can be done, and recognize them as unreasonable, it’s pretty common to try to personally move the barriers out of the way.

The thing is, though, we can’t do everything, and in large established institutions — and especially in broad multi-institutional collaborations — a lot of things are just not within our power to control.   Yes, we can slowly over time eke out change (#[144](https://www.researchcomputingteams.org/newsletter_issues/0144), Hacking your Bureaucracy), but not all at once, and not without a lot of help.

Leto advocates for some frank acknowledgement of our various “spheres” - our spheres of control (what we actually directly change), our spheres of influence (where we can advocate, support, and help, but not directly change), and, well - everything else, the sphere of concern, where things lie that can affect us but we can’t affect.   Writing down what’s in each, she argues, makes it much easier to focus attention where it can actually do some good, and to set aside what we can’t change or fix.

----------

[Take Aim: How To Reflect, Set Direction, and Make Progress In The Year Ahead](https://ashleyjanssen.com/take-aim-how-to-reflect-set-direction-and-make-progress-in-the-year-ahead/) - Ashley Janssen

So I’m doing advent of code again this year - a popular annual programming problem-fest.

Last year, my goal was “practice my long-out-of-date C++”.  This year, my goal is “I’m really excited about where the C++ standard is going for parallel execution, but to make use of it I’d need to better understand C++20 ranges, and C++17/C++20 algorithms, so I’m going to tackle each problem using those as much as I can manage”.

Guess which one is proving more successful at helping me develop my skills?

Having a more specific goal, with a reason *why* I’m targeting that goal, is turning out *much* better at driving my skill growth.  Already at day 10, my day 1 fumbling looks a little embarrassing.  I suspect the day 9 and 10 solutions I’m currently so pleased with will look equally cringey by day 25.  And yes, as a side effect I’m improving in other areas of C++, as well.

Janssen advocates taking a similar approach to personal goal setting for growth in the year ahead.

In our electronic and remote- or hybrid-work world, we generate a wealth of digital detritus - calendars, emails, documents - we can quickly review to remind us of the year that’s winding down.  That provides us ample material for reflection.

She then advocates planting your feet in a direction, setting the direction you want to grow in, along with the reason why.

Only then does she suggest planning some processes to put into place to move you in the right direction - taking into account people who can help, and small actionable steps that can be taken to move you in the right direction.

A crucial thing she emphasizes - it’s true that the specific goal without some initial steps to get there is just wishful thinking, but *it’s the goal that’s important,* not the steps.  As time goes on the steps can change and that’s fine.  But a specific goal (including a why) and an initial plan are powerful together.

----------

## Product Management and Working with Research Communities

[Research Application Managers: Overview](https://the-turing-way.netlify.app/collaboration/research-infrastructure-roles/ram.html) - The Turing Way

I’ve mentioned Turing’s great living [handbook](https://the-turing-way.netlify.app/welcome.html) before, which covers topics like reproducible research, project design, and more.

It also has a section on [research infrastructure roles](https://the-turing-way.netlify.app/collaboration/research-infrastructure-roles.html), which includes overviews of key roles like data stewards, community managers, and research software engineers.

Reader Michelle Barker, director of [ReSA](https://www.researchsoft.org), emailed me to point out new kind of role listed on the Turing’ Way handbook, one that is starting to be actively discussed elsewhere: the Research Application Manager.  (“Application” here is in the sense of application of new knowledge or result to a problem elsewhere, not as in a software application).

The problem that funding the role of RAM is meant to address is, quoting from the handbook:

<blockquote>
Traditionally, academia is less interested in supporting and rewarding work on:

<ul>
<li>Improving and extending existing research outputs/software</li>
<li>Promoting interoperability of new and existing outputs/software</li>
<li>Investing in usability, re-usability and user-friendliness of outputs/software (new and existing)</li>
<li>Co-creating outputs with users from the early stages of the research output lifecycle</li>
<li>Proactively discovering new real-world applications and use cases beyond the original academic field and investing in their promotion, adaptation and adoption</li>
</ul>

The problems RAMs are aiming to solve are ones that come up frequently in this newsletter!  Back in #[119](https://www.researchcomputingteams.org/newsletter_issues/0119) I talked about RCD research versus RCD development, and that translating research outputs into inputs in other area - moving things along the “technological readiness ladder” (#[91](https://www.researchcomputingteams.org/newsletter_issues/0091)) - is an important but underfunded responsibility.

In RCD, our mission is to scale our impact on research and beyond in our communities, while being bound to limited resources.  These technology transfer (or research translation, or knowledge transfer, or knowledge mobilization — different fields use different terms) efforts can have substantial impact if they’re done well, by people who understand both the research and the communities that might find it useful.

The Turing Way handbook outlines some key measures for success in RAMs, including:

<blockquote>
<ul>
<li>Engaging with the research team early on in the project to bring the perspective of potential users of their software tools and to proactively co-create from the early stages</li>
<li>[…]</li>
<li>Promoting the tools outside the academic field of the original research team</li>
<li>Approaching the output as a research “product” and bringing an appropriate level of “market intelligence” to the academic team</li>
<li>“Packaging” or “re-packaging” the tool to improve usability/accessibility to different audiences</li>
</ul>
</blockquote>

As described this is a pretty creative role, requiring people who can deeply engage with researchers and with potential client communities, and can “think product” as well as direct projects.   I’m pretty excited by the fact that this kind of role is being taken increasingly seriously, and I’ll be keeping an eye out for other organizations adopting it.

Have you seen other kinds of teams in your institution do this sort of work?  What are such teams called, and how are they funded?  How do they measure success?   Drop me a line and let me know - hit reply, or email jonathan@researchcomputingteams.org.

----------

## Research Software Development

[Active knowledge in software development](https://stayrelevant.globant.com/en/technology/agile-delivery/active-knowledge-in-software-development/) - Michael Feathers

Feather’s article argues that knowledge fades and goes stale.  On the other hand, re-working with a piece of a code base re-activates the knowledge in the mind of the team member working on it.

The consequence of that is when we’re thinking about knowledge preservation and transfer on teams, we can usefully be thinking not just in terms of documentation but in knowledge activation - what knowledge is currently loaded into someone’s cache, and how to share that while the knowledge is active (through discussion, pair programming, or something else).

> When we look at all of these qualities together, the most important thing to realize is that knowledge management should not be focused on conservation and storage. Instead, it should focus on generation and flow. Over time, knowledge dissipates, and people move from place to place. We need to concentrate on developing practices that help us rapidly acquire and generate knowledge on-demand to support the valuable work that we do in systems.

In retrospect this would have been a good article to pair with last issue’s (#[148](https://www.researchcomputingteams.org/newsletter_issues/0148)) post by Simon Willison about having a large number of projects.   There, the approach was to keep work as self contained as possible, so that it was easy to pick up again.  In Feathers’ language, that’s about making it easy to re-activate the knowledge.

----------

## Research Data Management and Analysis

[From Zero to 50 Million Uploads per Day: Scaling Media at Canva](https://canvatechblog.com/from-zero-to-50-million-uploads-per-day-scaling-media-at-canva-c81fa0c92f34) - Robert Sharp, Jacky Chen

I always love a good migration story.  Here, Sharp and Chen from Canva talk about the history of their storage solution for media content (management of stock photos and graphics that get used in graphic design work) a the company grew.

I admire how resolutely pragmatic the solutions they present have been - originally it was just a good old boring MySQL database.  Eventually they added some additional tooling to make schema migrations easier, but kept the same foundation.  They only started moving away from the simple approach when it was becoming truly untenable (schema migrations took six weeks!).  They then implemented a simple sharding approach to stave off the worst problems, and started prototyping a number of different longer-term solutions.

Finally, once an approach was decided on, they did a staged migration, starting new writes to the new data store, then moving the old data across, before finally shutting down the old data store.

There’s a lot of advantages to what they had done.  Foremost to my mind - by the time they moved to something non-boring, their data models were very mature and their usage patterns well understood, so they had a really good understanding of what they truly needed.  (No one ever knows what implementations actually at the beginning of a product’s life).

In their own words, some of their biggest lessons learned:

<blockquote>
<ul>
<li>Be lazy. Understand your access patterns, and migrate commonly accessed data first if you can.</li>
<li>Do it live. Gather as much information upfront as possible by migrating live, identifying bugs early, and learning to use and run the technology.</li>
<li>Test in production. The data in production is always more interesting than in test environments, so introduce checks in production where you can.</li>
</ul>
</blockquote>

----------

## Random

All models are wrong, but some are useful, even though they’re **very** **very** wrong.  Carnot’s absolutely foundational work on thermodynamics could not yet benefit from modern conceptions of energy.  It was instead [deeply informed by the bonkers “caloric” theory of heat](https://antonhowes.substack.com/p/age-of-invention-counting-caloric), which posited that heat was an actual material substance called [caloric](https://en.wikipedia.org/wiki/Caloric_theory) which repelled itself and so flowed out of hot things.  And yet that intuition produced the correct results, which in turn profoundly and positively affected the history of physics and engineering.

[Using generative AI to help come up with ideas](https://oneusefulthing.substack.com/p/how-to-use-ai-to-generate-ideas).

Really nice [set of lessons on Natural Language Processing](https://www.nlpdemystified.org), going from nothing to quite sophisticated.

The IBM PC adopting the 8086 wasn’t a foregone conclusion.  Other manufacturers, like TI and Motorola, had arguably better choices.  [How TI tried and failed to make it’s TMS990 the basis of the PC revolution](https://spectrum.ieee.org/the-inside-story-of-texas-instruments-biggest-blunder-the-tms9900-microprocessor).

Enjoy a relaxing holidays with the peace of mind that comes from having full test-suite coverage of your awk scripts (your awk scripts have test suites, right?) using [goawk’s cover mode](https://benhoyt.com/writings/goawk-coverage/).

A [Christmas tree ornament that plays Doom](https://spritesmods.com/?art=doom-bauble&page=1).

This is cool - another python ahead-of-time compiler, [codon](https://github.com/exaloop/codon), which looks quite sophisticated, and supports parallel threaded or gpu execution.

Create commit messages using ChatGPT with [commitgpt](https://github.com/RomanHotsiy/commitgpt).

Quick tutorial on generating small linux distributions for embedded devices [using buildroot](https://marcocetica.com/posts/buildroot-tutorial/).

[How the Precision Time Protocol is being deployed at Meta](https://engineering.fb.com/2022/11/21/production-engineering/precision-time-protocol-at-meta/).

[How cross-platform container environments work](https://iximiuz.ck.page/posts/container-tools-tips-and-tricks-issue-3).

----------

## That’s it…

And that’s it for another week.  Let me know what you thought, or if you have anything you’d like to share about the newsletter or management.  Just [email me](mailto:jonathan@researchcomputingteams.org) or reply to this newsletter if you get it in your inbox.

Have a great weekend, and good luck in the coming week with your research computing team,

Jonathan

### About This Newsletter

Research computing - the intertwined streams of software development, systems, data management and analysis - is much more than technology.  It’s teams, it’s communities, it’s product management - it’s people.  It’s also one of the most important ways we can be supporting science, scholarship, and R&D today.

So research computing teams are too important to research to be managed poorly.  But no one teaches us how to be effective managers and leaders in academia.  We have an advantage, though - working in research collaborations have taught us the advanced management skills, but not the basics.

This newsletter focusses on providing new and experienced research computing and data managers the tools they need to be good managers without the stress, and to help their teams achieve great results and grow their careers.

----------

## Jobs Leading Research Computing Teams

This week’s new-listing highlights are below in the email edition; the full listing of 196 jobs is, as ever, available on [the job board](https://www.researchcomputingteams.org/jobs/).

**[Compute Lead, Technical Services Cluster](https://www.embl.org/jobs/position/EBI02108)** - EMBL-EBI, Hinxton UK <br/>
We are seeking a highly motivated and experienced HPC professional to lead our Compute team within our Technical Services Cluster (TSC), serving an institute of over 800 researchers and technical staff. You will be working closely with members of the TSC, and more widely with technical users across EMBL-EBI, overseeing the design, deployment and operation of our HPC clusters. The EMBL-EBI has multiple large HPC clusters to support the large data flows and processing services fundamental to EMBL-EBI activities. The services you will provide, along with the TSC, enable EMBL-EBI’s technical research and administrative staff to process and make the world’s public biological data freely available to the scientific community via a range of services and tools, perform basic research and provide professional training in bioinformatics.

**[Manager, High Performance Computing](https://opportunities.columbia.edu/jobs/manager-high-performance-computing-hpc-manhattanville-new-york-united-states)** - Columbia University, New York NY USA <br/>
Reporting to the Sr. Director, Research Services; the Manager of High-Performance Computing (HPC) leads the HPC team and interacts extensively with leaders of other research support groups including the Office of the Executive VP for Research and other CUIT groups. Key initiatives to be accomplished by the incumbent include advancement of a shared High Performance Computing platform, advancement of research computing in the cloud, assessment and development of research tools for computational performance. Additionally, the Manager leads collaboration efforts with other central research support entities at Columbia University.

**[Software Development Manager - Compilation](https://xanadu.applytojob.com/apply/iA080Sm6uK/Software-Development-Manager-Compilation)** - Xanadu, Toronto ON CA <br/>
Xanadu is looking for an experienced Software Development Manager to lead the Quantum Compilation team. The team is developing JIT and AOT hybrid compilation pipelines for PennyLane, an open-source library for quantum machine learning, quantum computing, and quantum chemistry. Although quantum software development experience is not required for the role, a strong compilation background is strongly preferred.

**[Software Development Manager - Quantum Software](https://xanadu.applytojob.com/apply/jvLkznTUaV/Software-Development-Manager-Quantum-Software)** - Xanadu, Toronto ON CA <br/>
Xanadu is looking for an experienced Software Development Manager to lead the Core Quantum Software team. The team is developing PennyLane, an open-source framework for quantum machine learning, quantum computing, and quantum chemistry. Although quantum software development experience is not required for the role, an advanced degree in physics, math or computer science is preferred.

**[Software Development Manager - Hardware Infrastructure](https://www.linkedin.com/jobs/view/software-development-manager-hardware-infrastructure-at-xanadu-3388314435/?originalSubdomain=ca)** - Xanadu, Toronto ON CA <br/>
Xanadu’s mission is to build quantum computers that are useful and available to people everywhere.  Xanadu is looking for an experienced Software Development Manager to lead the Infrastructure team. The team’s mission is to accelerate hardware timelines by providing software support. This is a key role for our organization as we work towards designing and building a commercially viable fault-tolerant quantum computer.

**[Associate Director for Research Development and Data Support, Penn Carey Law School](https://wd1.myworkdaysite.com/en-US/recruiting/upenn/careers-at-penn/details/Associate-Director-for-Research-Development-and-Data-Support--Hybrid-Eligible-_JR00059042?q=biddle)** - University of Pennsylvania, Philadelphia PA USA <br/>
The University of Pennsylvania Carey Law School seeks the inaugural Associate Director for Research Development and Data Support to launch the Data and Impact Hub. The Data and Impact Hub will provide open access to Penn Carey Law data and scholarship. The hub will support the full empirical research cycle – helping law faculty develop and administer studies, supporting data analysis, spotlighting and preserving research output, and increasing the ongoing impact and reach of Penn Carey Law scholarship.

**[Product Manager, Ocean Vision AI](https://www.oceandiscoveryleague.org/jobs/product-manager)** - Ocean Discovery League, Remote USA <br/>
Ocean Vision AI is an NSF-funded effort to address the extraordinary challenge of scaling up our capacity to analyze underwater visual data. The Ocean Vision AI Platform is a tool designed to dramatically accelerate the analysis of ocean video to localize and classify the organisms found within video frames. While the end-goal will be for the tool to do this automatically, the interim steps will involve a product that allows for training and correction of algorithms from users of various experience levels. The Product Manager serves as a key team member within the project and will be responsible for requirements gathering and prioritization of features of the Ocean Vision AI platform. The Product Manager will collaborate with key team members and stakeholders to support and help create a compelling product strategy and roadmap and help drive product features from concept to launch.

**[AI research and development (R&D) Lead](https://www.linkedin.com/jobs/view/ai-research-and-development-r-d-lead-at-wine-valet-3377691520/?originalSubdomain=au)** - Wine Valet, Remote AU <br/>
wineries all around Australia. The app learns customer taste profiles to provide AI-generated wine recommendations. Wine Valet is now looking for an AI Research and Development Lead to join our fast-growing team.  As the AI Research and Development (R&D) Lead, you will be given the perfect balance of responsibility and support to lead the R&D project which underpins the wine recommendation engine of the Wine Valet app. You will be responsible for analysis of the data the Wine Valet app collects and for developing models and algorithms that apply artificial intelligence for optimising customer wine recommendations. You will also work closely with the executive team and the R&D team to ensure the AI algorithm is optimised for the best customer experience.

**[Data Scientist - Head of Radiology AI](https://careers.dana-farber.org/job/data-scientist-head-of-radiology-ai-research-laboratory-boston-ma-31735/)** - Dana-Farber Cancer Institute, Boston MA USA <br/>
We are seeking an intelligent, hard-working, and dynamic individual to serve as the Radiology AI area leader and senior data scientist within the Artificial Intelligence Operations and Data Science Services group (AIOS) in the Informatics & Analytics department of Dana-Farber Cancer Institute – a teaching affiliate of Harvard Medical School. The team works on both short-term priorities identified by our top clinicians, as well as on long-term institutional efforts that aim at revolutionizing the way the Institute conducts basic cancer research and provides best-in-class clinical oncology to our patients. The AIOS group encompasses expertise in AI, data science, machine learning, computer vision, NLP, production deployment, cloud infrastructure, data engineering, project management standards, and data labeling.

**[Software Project Manager](https://www.linkedin.com/jobs/view/software-project-manager-at-oxford-quantum-circuits-3390254356/?originalSubdomain=uk)** - Oxford Quantum Circuits, Oxford UK <br/>
At Oxford Quantum Circuits (OQC) we build quantum computers to enable life-changing discoveries: from new drug modelisation to longer-lasting battery technology and portfolio optimisation. As a Software Project Manager at OQC, you will join a team of physicists and software engineers developing amazing solutions to fuel the quantum computing revolution. You will be fully integrated into the Software Team and will be working alongside other teams of engineers and physicists. You will be able to make full use of your problem solving and analytical skills to develop high-performance software solutions to enable OQC’s quantum hardware technologies.

**[Head of Computing, School of Informatics](https://elxw.fa.em3.oraclecloud.com/hcmUI/CandidateExperience/en/sites/CX_1001/job/5987)** - University of Edinburgh, Edinburgh UK <br/>
We are inviting applications for a Head of Computing to provide strategic leadership and management of the School’s Computing Services team which includes the following units - Services; Systems; User Support and Procurement; Research and Teaching.  The School’s Computing team support and run services for over 250 academic, research and teaching staff, over 80 professional services staff, 450 research postgraduate students, over 700 taught postgraduate students, 1400 undergraduate students and over 350 visitors. The team manages  ~450 Linux desktops, ~100 Windows desktops/laptops, 320 physical and 250 virtual servers (hosting over 700 GPUs), supports ~600 self managed laptops, desktops and servers,  and develops solutions to ensure the School’s academic and research staff can undertake their research and teaching as efficiently as possible. The successful candidate will provide technical leadership to the Computing team, and will lead the development of overall strategy and management across computing services with the School’s Computing Strategy Group.

**[HPC Team Leader](https://jobs.csiro.au/job/Melbourne%2C-VIC-HPC-Team-Leader/934328810/)** - CSIRO, Melbourne or Sydney or Canberra AU <br/>
The Team Leader (CSOF6) roles within Scientific Computing Platforms, are management positions within the Information Management & Technology (IMT) function, reporting to the Executive Manager, Scientific Computing Platforms. The Team Leader provides highly skilled industry applied knowledge ensuring that service implementation is achieved through team planning, configuration, testing and interoperability of the solutions; and ensuring the platforms/services are professionally managed to support CSIRO in a challenging and dynamic environment.

**[Research Manager, Machine Learning Group](https://www.linkedin.com/jobs/view/research-manager-at-university-of-guelph-3381339840/?originalSubdomain=ca)** - University of Guelph, Guelph ON CA <br/>
The Machine Learning Research Group (MLRG) at the University of Guelph (U of G) is a large and vibrant lab engaged in cutting edge research spanning a number of topics in deep learning (DL). MLRG is known across Canada for its strength and balance between fundamental and applied DL. Over the past several years the lab has become an increasingly multidisciplinary team recruiting computer scientists, computer, electrical and biomedical engineers, psychologists and biologists. Together, the lab members pursue challenging problems and large-scale projects at the intersection of DL and other disciplines such as biodiversity science. This includes collaborations with the Vector Institute (Vector) and growing collaborations with international initiatives such as BIOSCAN and LIFEPLAN.  As Research Manager of MLRG, you will report directly to the PI, Dr. Graham Taylor, a Canada Research Chair in Machine Learning. In addition to heading his lab, Dr. Taylor is the Research Director at the Vector Institute, the program director for the Collaborative Specialization in Artificial Intelligence, and an academic co-director at Guelph's Centre for Advancing Responsible and Ethical AI. Your key role as Research Manager is to provide on-the-ground assistance with lab and research activities, experience that is typically gained through graduate school. You’ll play a central role in advancing the team’s impact and influence.

**[Group Product Manager](https://www.linkedin.com/jobs/view/group-product-manager-at-borealis-ai-3391688453/?originalSubdomain=ca)** - Borealis AI, Montreal QC or Toronto or Waterloo ON or Vancouver BC CA <br/>
As a Group Product Manager on the Product team, you’ll get a chance to lead the development of a state-of-the-art AI product for the financial services industry. You and the cross-functional project team will be responsible for discovering, building and delivering a product that solves a high-impact business problem. You will also be responsible for defining the long-term product strategy, roadmap, and objectives to lead your team towards a product vision that supports both Borealis AI’s and RBC strategic directions. At Borealis AI, you’ll be joining a team that works directly with leading machine learning researchers and software developers. Our mission is to think big and go beyond what’s possible today to deliver competitive advantage to RBC through AI product development. This is a rare opportunity to build the skills required to make machine learning work in the enterprise.

**[Senior Technical Project Manager - Remote](https://www.linkedin.com/jobs/view/senior-technical-project-manager-remote-at-real-chemistry-3379855083/?originalSubdomain=ca)** - Real Chemistry, Toronto ON or Remote CA <br/>
Real Chemistry creates the world around modern therapies with over 2,000 talented professionals, and for the last 20+ years has, carved out its space at the intersection between healthcare, marketing and communications, data & AI, and the people at the heart of it all.  We are seeking a talented Senior Technical Project Manager for conversationHEALTH (cH), a division of Real Chemistry. The Senior Technical Project Manager (Sr. TPM) oversees a portfolio of projects from conceptual design through implementation by collaborating with clients, vendors, and in-house personnel. Additionally Sr. TPM is the catalyst to form a team of technical experts to deliver high quality solutions to our clients while maximizing the utilization of resources and constantly improving efficiency.

**[Senior Software Engineer, Workflows, Centre for Population Genomics](https://populationgenomics.org.au/wp-content/uploads/2022/11/CPG-Senior-Software-Engineer-Position-Production-Pipelines-Description.pdf)** - Garvan Institute of Medical Research,, Sydney or Melbourne AU <br/>
The Centre for Population Genomics focuses on the development of cutting-edge tools and resources to facilitate the conversion of genomic data into improved diagnosis and treatment for Australians, the field known as genomic medicine. You will be responsible for leading the production workflows group within the software team, focussing on designing and implementing large scale genomic analysis methods (using and extending frameworks like Hail) that feed into web-based tools (like the gnomAD browser or seqr) for providing intuitive access to complex scientific datasets. This role will be a part of a small, high-energy software engineering team that will work in close collaboration with the Centre’s genomic analysis teams. You will lead the group that develops the workflow framework and infrastructure that’s used continuously by the Centre’s core pipelines used for large cohort and rare disease analysis, iterating with the computational biologists and population geneticists in the analysis teams on the productionization of their analysis methods.

**[Statistical Science Director / Statistical Science Associate Director (Principal level) / Senior Statistician – Early Cardiovascular, Renal, and Metabolism](https://www.linkedin.com/jobs/view/statistical-science-director-statistical-science-associate-director-principal-level-senior-statistician-%E2%80%93-early-cardiovascular-renal-and-metabolism-at-astrazeneca-3374635079/?originalSubdomain=ca)** - Astrazeneca, Mississauga ON CA <br/>
Do you have expertise in, and passion for, Biostatistics? Would you like to apply your skills to impact the early phases of drug development and regulatory interactions, in a company that follows the science and turns ideas into life changing medicines? Then AstraZeneca might be the one for you! We are currently recruiting for Statisticians at Director, Associate Director or Senior level depending on your previous experience to join our growing Early Clinical - Cardiovascular, Renal, and Metabolism (CVRM) portfolio. As part of the Data Science and AI organisation, you’ll use technology at the forefront of science in a creative environment, with the scope to develop new statistical ideas and approaches and apply them in your work.

**[Product Manager - High Performance Computing](https://careers.roche.com/global/en/job/ROCHGLOBAL202207127568EXTERNALENGLOBAL/Product-Manager-High-Performance-Computing)** - Roche, Various <br/>
As a Product Manager you will be accountable for product(s) vision, strategy, roadmap and end-to-end product lifecycle. You will promote a strong focus on Product Management including networked, agile ways of working and role model business partnering closely collaborating with stakeholders, Domain Leads and Product Line Leads to shape the digital strategy, roadmaps and portfolios and co-create solutions to maximize the value delivered by the agile teams within your product(s) area.  You’ll be working within the Engineering & Infrastructure Ecosystems Domain and Hosting Infrastructure Product Line. The Hosting Infrastructure product line provides a complete, fit for purpose set of hosting platforms including virtual machine farms, high performance compute clusters, Managed Container Orchestration platforms with availability in global data centers and selected services available in computer rooms at Roche sites, Global Data Center Management, Local Computer Rooms creation and operations services.

**[IT/Systems Manager, Gordon Center for Medical Imaging](https://partners.taleo.net/careersection/ghc/jobdetail.ftl?job=3212171&tz=GMT-05%3A00&tzname=America%2FToronto)** - Massacheusetts General Hospital, Boston MA USA <br/>
Receiving general direction from the Director of Gordon Center for Medical Imaging, the Computer Administrator is responsible for the administration of computer infrastructure and backup of user/instrument data in Gordon Center.  The Computer Administrator has a strong desire to apply his or her problem-solving skills in IT management to make essential contributions to the development of next-generation medical imaging technologies. The ideal candidate is a self-driven individual who enjoys working both independently and collaboratively and has a positive outlook. Experience in computer programming and AI, or an interest in developing skills in these areas, is preferred and provides opportunities for creative research directions. Install, configure, test, troubleshoot, upgrade, and maintain multiple Linux CPU/GPU clusters.

**[Manager, Health Informatics](https://eeho.fa.us2.oraclecloud.com/hcmUI/CandidateExperience/en/sites/CX_1/job/97105BR)** - Oracle, Springfield MO USA <br/>
As a first line management role, responsible for directing and reviewing the team’s work to achieve operational or functional targets or objectives with measurable contribution towards the achievement of immediate and short-term results for the team and department. Establish and communicate team goals that support and advance team and department objectives; establish metrics or other performance measures to track progress. Understand and explain policies, practices and procedures within immediate area of responsibility to stakeholders within and outside of the team. Identify, propose and lead team- and department-level quality and process improvement initiatives; participate and provide input on strategic initiatives at the department-level

**[Quantum Optics & AMO (Manager / Head)](https://www.linkedin.com/jobs/view/quantum-optics-amo-manager-head-at-nu-quantum-3387348210/?originalSubdomain=uk)** - Nu Quantum, Cambridge UK <br/>
Founded to commercialise research generated over the last decade at the Cavendish Laboratory, Nu Quantum is on a mission to shape the future of quantum information systems. We’re working on exciting technology that will improve the utility and accelerate the time-to-market of quantum-computing systems. In partnership with world-leading companies and academic groups we’re integrating novel quantum technology to form an efficient and scalable quantum networking infrastructure. Join a brilliant, diverse and multi-disciplined team to turn leading research into high value, market-defining solutions.

**[Associate Director, Product, AllOfUs](https://recruiting2.ultipro.com/SCR1003TSRI/JobBoard/98759e7d-7ede-4c0b-ac7b-2c6293c7b522/OpportunityDetail?opportunityId=809247b4-2411-4a7b-9f28-b9813374f325)** - Scripps Research Institute, Remote or La Jolla CA USA <br/>
We are looking for an Associate Director of Product with the experience and passion to lead a team in the creation and implementation of a range of user-centered digital solutions across our portfolio of digital health initiatives, with a focus on the All of Us Research Program (AoURP).  As an Associate Director of Product, you will be responsible for driving product initiatives from inception through execution by leading a team of designers and technologists and collaborating closely with biomedical researchers, behavioral scientists, data scientists, and other cross-functional team members within the organization to ensure that our products engage research participants and accelerate the process of digital health technologies.

**[AI/ML Centre of Excellence Lead, AI, Data, and Analytics](https://pfizer.wd1.myworkdayjobs.com/PfizerCareers/job/United-States---New-York---New-York-City/AI-ML-CoE-Lead--AIDA_4873704-1?source=linkedin)** - Pfizer, Various USA <br/>
The AI/ML CoE (Center of Excellence) Lead will foster the development of a world-class AI/ML CoE for Pfizer. Utilizing their deep expertise in Artificial Intelligence, Machine Learning, Deep Learning, and other advanced analytics capabilities, the CoE Lead will serve as an innovative leader, who inspires and educates the broader Pfizer community on “the art of the possible” and drives the adoption of AI/ML to advance Pfizer’s business goals and competitive advantage. In addition to algorithm development, the AI/ML CoE Lead will grow expert analytics capabilities in NLP, Image analysis, and Acoustic data analysis. This role will also lead the broader Pfizer AI community in developing tools for model standardization, documentation, and reuse, and ensure ethical, responsible, and understandable AI usage.

**[Lead, Machine Learning Research (Director)](https://pfizer.wd1.myworkdayjobs.com/PfizerCareers/job/United-States---Massachusetts---Cambridge/Lead--Machine-Learning-Research--Director-_4873702-2?source=linkedin)** - Pfizer, Cambridge MA or La Jolla CA or Groton CT USA <br/>
Pfizer's Machine Learning Research department in our Worldwide Research, Development and Medical (WRDM) organization is seeking a passionate, creative, and experienced machine learning scientist to lead the development and implementation of cutting-edge machine learning tools to accelerate the most difficult drug discovery challenges. The ideal candidate should have an outstanding scientific reputation in the field of machine learning research, be very familiar with the drug discovery processes, have a solid life science relevant domain knowledge and be passionate about visionary research strategies. They will work in close collaboration with groups across Research Units to address challenging problems such as target discovery.
